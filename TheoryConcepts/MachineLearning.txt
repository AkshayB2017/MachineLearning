Understanding Machine Learning- Theory to Algorithms

Chapter 1

Bait Shyness- Rats Learning to Avoid Poisonous Baits: Rats eat small amounts of new food items and if there is an ill effect associated with it, rats will avoid it. Animals use past experience to acquire expertise in detecting safety of food.

This can be applied in email-spam labelling question, but here the question is how will the spam detector react to a new, unique case which was not part of the training set.

A successful learner should be able to progress from individual
examples to broader generalization. This is also referred to as inductive reasoning
or inductive inference.

Pigeon Superstition- Consider a bunch of hungry pigeons in a cage. Food was delivered to the pigeons without reference to their behaviour. Each bird started reinforcing the action that they did when they got the food and kept repeating in the expectation of food ariving. Which is not useful learning.

Bait Shyness 2- Due to previous knowledge, the birds were able to realise that there is no correlation between eating food and getting electic shock despite the fact that birds got electric shock when they consumed food. This concept is inductive bias.

When do we need Machine Learning?

1. Task to be done is too complex
   a. Tasks performed by humans
         Driving, Speech Recognition, Image Understanding
   b. Tasks beyond human capabilities
            astronomical data, turning medical archives into medical knowledge, weather prediction, analysis of genomic data, Web search engines, and electronic commerce.

One limiting feature of programmed tools is their rigidity – once the program has been written down and installed, it stays unchanged. However, many tasks change over time or from one user to another.
Machine learning tools – programs whose behavior adapts to their input data – offer a solution to such issues; they are, by nature, adaptive to changes in the environment they interact with.

The first distinction to note is the difference between supervised and unsupervised learning.
 Consider the task of learning to detect spam e-mail versus the task of anomaly detection. For the spam detection task, we consider a setting in which the learner receives training e-mails for which the label spam/not-spam is provided. On the basis of such training the learner should figure out a rule for labeling a newly arriving e-mail message. In contrast, for the task of anomaly detection, all the learner gets as training is a large body of e-mail messages (with no labels) and the learner’s task is to detect “unusual” messages.

 We distinguish between “active” and “passive” learners. An active learner interacts with the environment at training time, say, by posing queries or performing experiments, while a passive learner only observes the information provided by the environment (or the teacher) without influencing or directing it.

When one thinks about human learning, of a baby at home or a student at school, the process often involves a helpful teacher, who is trying to feed the learner with the information most useful for achieving the learning goal. In contrast, when a scientist learns about nature, the environment, playing the role of the teacher, can be best thought of as passive – apples drop, stars shine, and the rain fallswithout regard to the needs of the learner. We model such learning scenarios by postulating that the training data (or the learner’s experience )is generated by some random process. This is the basic building block in the branch of “statistical learning.” Finally, learning also occurs when the learner’s input is generated by an adversarial “teacher.”

The distinction between situations in which the learner has to respond online, throughout the learning process, and settings in which the learner has to engage the acquired expertise only after having a chance to process large amounts of data.
